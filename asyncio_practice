import aiohttp
import asyncio
import json
import time
import os
from bs4 import BeautifulSoup
from packageurl import PackageURL

output_dir = r"C:\Users\Sameer Bhagavatula\Documents\VS Code\Securin Intership\ASYNCIO PRACTICE FILES"
semaphore = asyncio.Semaphore(100)

async def fetch(session, url):
    async with semaphore, session.get(url) as response:
        return await response.text()

async def get_package_keywords(session, package_name):
    url = f"https://pypi.org/project/{package_name}"
    html = await fetch(session, url)
    soup = BeautifulSoup(html, "html.parser")
    meta = soup.find("p", {"class": "package-header__meta"})
    if meta:
        keywords = [a.text for a in meta.find_all("a")]
    else:
        keywords = []

    return keywords

async def get_package_info(session, package_name):
    url = f"https://pypi.org/pypi/{package_name}/json"
    async with semaphore, session.get(url) as response:
        if response.status != 200:
            print(
                f"Failed to get info for package {package_name}: HTTP {response.status}"
            )
            return None

        data = await response.json()

        if "info" not in data:
            print(
                f"Failed to get info for package {package_name}: 'info' key not found in response"
            )
            return None

        info = data["info"]
        project_urls = info.get("project_urls")
        source_repository = (
            project_urls.get("Homepage") if project_urls is not None else None
        )
        
        purl = PackageURL(type='pypi', name=info.get("name"), version=info.get("version")).to_string()

        return {
            "name": info.get("name"),
            "version": info.get("version"),
            "source_repository": source_repository,
            "author": info.get("author"),
            "author_email": info.get("author_email"),
            "license": info.get("license"),
            "purl": purl,
            "keywords": await get_package_keywords(session, package_name),
        }

async def main():
    url = "https://pypi.org/simple/"
    failed_packages = []
    async with aiohttp.ClientSession() as session:
        html = await fetch(session, url)
        soup = BeautifulSoup(html, "html.parser")
        package_links = [link.get("href") for link in soup.find_all("a")]
        package_names = [link.split("/")[-2] for link in package_links]

        for i in range(0, len(package_names), 100):
            chunk = package_names[i:i+100]
            package_infos = await asyncio.gather(
                *[get_package_info(session, package_name) for package_name in chunk]
            )

            for package_name, package_info in zip(chunk, package_infos):
                if package_info is not None:
                    with open(os.path.join(output_dir, f"{package_info['name']}.json"), "w") as f:
                        json.dump(package_info, f)
                else:
                    failed_packages.append(package_name)

            await asyncio.sleep(1)  # to avoid rate limiting

        with open(os.path.join(output_dir, "failed_packages.json"), "w") as f:
            json.dump(failed_packages, f)

if __name__ == "__main__":
    start_time = time.time()
    asyncio.run(main())
    end_time = time.time()
    print(f"Runtime: {end_time - start_time} seconds")